

# IRL on Duckietown

This repo implements Simple ICRL for lane following in Duckietown. For more details on the method and implementation, see our [blog post](https://ahugs.github.io/blog/2024/lane_following). 

## Implementation 
### RL Method

Implementation of DrQ-v2 taken from original [repo](https://github.com/facebookresearch/drqv2).

[[Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning]](https://arxiv.org/abs/2107.09645)

### IRL Method
We implement a version of [Maximum Entropy IRL](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf). For partial IRL,
we follow the procedure outlined in [Simplying Constraint Inference With Inverse Reinforcement Learning](https://openreview.net/forum?id=T5Cerv7PT2), including L2 regularization on the expert rewards.

### Evaluation
The evaluation code is adapted from the module `DuckietownWorldEvaluator` in this [repo](https://github.com/kaland313/Duckietown-RL/tree/420b7f064321b9b98edf1900a013562388a15101)
which accompanies the paper [Vision-based Reinforcement learning for lane-tracking control](https://acta.imeko.org/index.php/acta-imeko/article/view/IMEKO-ACTA-10%20%282021%29-03-04)

## Reproducing Results

### Environment Setup

Build and start the docker container
```
docker build -f Dockerfile -t duckietown-irl:latest .
docker run -e WANDB_API_KEY={WANDB_API_KEY}  -v $(pwd):/workspaces/duckietown-irl -w /workspaces/duckietown-irl --gpus all -it duckietown-irl:latest
```
If you do not want to log to WandB, you can add `-e WANDB_MODE=disabled` to the `docker run` command


If you would like to use the same expert trajectories used for our experiments, they can be downloaded from [here](https://drive.google.com/drive/folders/10WS2OVGey_lDfIpUaCDS3sEdt6-iS154?usp=sharing).

They should be saved to a folder in the top level of this repository called `data/{MAP_NAME}` (or otherwise the config files should be adjusted appropriately)

Otherwise, they can be generated by running:

```
python scripts/generate_expert_trajs.py
```

### Running experiments 
To run the base experiment (i.e. ICRL with velocity reward on small loop) do (from inside docker container)

```
python scripts/train_irl.py seed={SEED}
```

To run IRL with no reward, human-designed reward RL or velocity-only reward RL. you can change the second line to

```python scripts/train_irl.py +experiment=full_irl seed={SEED}```

```python scripts/train_irl.py +experiment=rl_human seed={SEED}```

```python scripts/train_irl.py +experiment=rl_velocity seed={SEED}```

Our experiments were run over three seeds (0,1,2)

To run the same experiments on a mixture of maps, you can add the following arguments to these commands:
`env@train_env=multi_map_duckietown`, `env@eval_env=multi_map_duckietown_eval`, `expert_dir=data/mixed`.

Make sure to first generate or download the expert trajectories to `data/mixed`.

### Evaluation (in simulation)

By default, your policies will save to a folder under `exp_local` corresponding to the date and time the experiment was launched. To run evaluation of a single policy, you can run:
```
python scripts/evaluate_policy.py outdir={DIRECTORY_TO_SAVE_EVAL_RESULTS} policy_path={PATH_TO_A_TRAINED_POLICY}
```

If you have trained all four of the policies above (or downloaded our saved policies [here](https://drive.google.com/drive/folders/1iFTrxPvGOaEYerk0U_xlKiAROyqwC2PU?usp=sharing)), you can reproduce the evaluation plots and videos by running:

```
./run_eval.sh
```

or (for mixed map training)
```
./run_eval_mixed.sh
```

and then running the notebook `eval_results/analysis.ipynb` once the evaluations have completed. 

Before running the evaluation, you should ensure that you create a folder `policies` which has the following directory structure: `polcies/{MAP}/{EXPERIMENT_NAME}/{SEED}/snapshot.pt` (i.e. in `policies/small_loop/rl_human/0` there should be a file `snapshot.pt` which is a trained policy for seed 0 on the experiment RL with human-designed reward in the small loop map). In the notebook, you can set the variable `map` to correspond to the above `{MAP}` where you saved the policies.

Of course, you can name your experiments other things or follow a different directory structure for reading/writing but you will need to update `run_eval.sh` which sets arguments `outdir` and `policy_path` to the defaults for our experiments.

### Running on Duckiebot

You can deploy an RL policy trained in this repo on a real Duckiebot using the [duckietown-deploy](https://github.com/ahugs/duckietown-deploy/tree/ente) repo. Make sure you are on the `ente` branch. This assumes you have setup the Duckietown development environment on your machine, if not, follow the directions [here](https://docs.duckietown.com/ente/opmanual-duckiebot/setup/setup_laptop/index.html).


1. Save the actor and encoder weights for the trained policy. You can do this by running the following in a `python` terminal:

    ``` 
    import torch
    import src.agents.drqv2
    agent = torch.load({POLICY_PATH})['agent']
    torch.save(agent.encoder.state_dict(), {ENCODER_WEIGHTS_SAVE_PATH})
    torch.save(agent.actor.state_dict(), {ACTOR_WEIGHTS_SAVE_PATH})
    ``` 

2. Clone the repo 

    ```
    git clone git@github.com:ahugs/duckietown-deploy.git
    ```

3. Put your saved model weights in the top level of this repo (should be named `encoder_weights.pth` and `actor_weights.pth`, respectively)

4. Build the code:
    ```
    dts devel build
    ```
5. (If running in the Duckiematrix):
    ```
    dts duckiebot virtual start [ROBOT_NAME]
    dts matrix run --standalone --sandbox
    dts matrix attach [ROBOT_NAME] map_0/vehicle_0
    ```
5. Run:
    ```
    dts devel run -R [ROBOT_NAME]
    ```
