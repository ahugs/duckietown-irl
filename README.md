

# IRL on Duckietown

This repo implements Simple ICRL for lane following in Duckietown. For more details on the method and implementation, see our [blog post](https://ahugs.github.io/blog/2024/lane_following). 

## Implementation 
### RL Method

Implementation of DrQ-v2 taken from original [repo](https://github.com/facebookresearch/drqv2).

[[Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning]](https://arxiv.org/abs/2107.09645)

### IRL Method
We implement a version of [Maximum Entropy IRL](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf). For partial IRL,
we follow the procedure outlined in [Simplying Constraint Inference With Inverse Reinforcement Learning](https://openreview.net/forum?id=T5Cerv7PT2), including L2 regularization on the expert rewards.

### Evaluation
The evaluation code is adapted from the module `DuckietownWorldEvaluator` in this [repo](https://github.com/kaland313/Duckietown-RL/tree/420b7f064321b9b98edf1900a013562388a15101)
which accompanie the paper [Vision-based Reinforcement learning for lane-tracking control](https://acta.imeko.org/index.php/acta-imeko/article/view/IMEKO-ACTA-10%20%282021%29-03-04)

## Reproducing Results

### Environment Setup

Build the docker container
```
docker build -f Dockerfile -t duckietown-irl:latest .
```

If you would like to use the same expert trajectories used for our experiments, they can be downloaded from [here](https://drive.google.com/drive/folders/1qB9dCyfNXDdcKx3UfDHMAonc-Q_Yp-cj?usp=sharing).

They should be saved to a folder in the top level of this repository called `data/{MAP_NAME}` (or otherwise the config files should be adjusted appropriately)

Otherwise, they can be generated by running:

```
docker run -e WANDB_API_KEY={WANDB_API_KEY}  -v $(pwd):/workspaces/ --gpus all -it duckietown-irl:latest
python scripts/generate_expert_trajs.py
```

### Running experiments 
To run the base experiment (i.e. ICRL with velocity reward) do

```
docker run -e WANDB_API_KEY={WANDB_API_KEY}  -v $(pwd):/workspaces/ --gpus all -it duckietown-irl:latest
python scripts/train_irl.py seed={SEED}
```

To run IRL with no reward, human-designed reward RL or velocity-only reward RL. you can change the second line to

```python scripts/train_irl.py +experiment=full_irl seed={SEED}```

```python scripts/train_irl.py +experiment=rl_human seed={SEED}```

```python scripts/train_irl.py +experiment=rl_velocity seed={SEED}```

Our experiments were run over three seeds (0,1,2)

### Evaluation (in simulation)

To run evaluation of a single policy, you can run:
```
python scripts/evaluate_policy.py outdir={DIRECTORY_TO_SAVE_EVAL_RESULTS} policy_path={PATH_TO_A_TRAINED_POLICY}
```

If you have trained all four of the policies above, you can reproduce the evaluation plots and videos by running:

```
chmod +x run_eval.sh
./run_eval.sh
```

and then running the notebook `eval_results/analysis.ipynb` once the evaluations have completed.

Before running the evaluation, you should ensure that you create a folder `policies` which has the following directory structure: `polcies/{EXPERIMENT_NAME}/{SEED}/snapshot.pt` (i.e. in `policies/rl_human/0` there should be a file `snapshot.pt` which is a trained policy for seed 0 on the experiment RL with human-designed reward). You should also make the same directory structure exists under `eval_results`. 

Of course, you can name your experiments other things or follow a different directory structure for reading/writing but you will need to update `run_eval.sh` which sets arguments `outdir` and `policy_path` to the defaults for our experiments.

### Running on Duckiebot

You can deploy an RL policy trained in this repo on a real Duckiebot using the [duckietown-deploy](https://github.com/ahugs/duckietown-deploy/tree/ente) repo. Make sure you are on the `ente` branch. This assumes you have setup the Duckietwon development environment on your machine, if not, follow the directions [here](https://docs.duckietown.com/ente/opmanual-duckiebot/setup/setup_laptop/index.html).


1. Save the actor and encoder weights for the trained policy. You can do this by running:

    ```torch.save({OBJECT}.state_dict(), {PATH})``` 

    where `OBJECT` is a `torch.nn.Module` object (the `Actor` or the `Encoder` object here) and `PATH` is the save path.

2. Clone the repo 

    ```
    git clone git@github.com:ahugs/duckietown-deploy.git
    ```

3. Put your saved model weights in the top level of this repo (should be named `encoder_weights.pth` and `actor_weights.pth`, respectively)

4. Build the code:
    ```
    dts devel build
    ```

5. Run:
    ```
    dts devel run -R {DUCKIE_NAME}
    ```
